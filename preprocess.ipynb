{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbc4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_PATH = \"Flickr8k/captions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5639cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_preprocessing():\n",
    "    items = dict()\n",
    "    with open(CAPTIONS_PATH, \"r\") as f:\n",
    "        raw_data = f.read()\n",
    "    f.close()\n",
    "    lines = raw_data.split(\"\\n\")\n",
    "    for line in lines[1:]:  # Skip the first line which is a header\n",
    "        if len(line) > 0:\n",
    "            img_path, caption = line.split(',', 1)\n",
    "            if img_path not in items:\n",
    "                items[img_path] = []\n",
    "            caption = caption.lower()\n",
    "            caption = caption.strip(\" .\")\n",
    "            # caption = '<START> ' + caption + ' <END>'\n",
    "            items[img_path].append(caption)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de78dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = caption_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f8772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for image in items:\n",
    "    for caption in items[image]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c53e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "max_len = 0\n",
    "for caption in all_captions:\n",
    "    max_len = max(max_len, len(caption.split(' ')))\n",
    "    for word in caption.split(' '):\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "min_freq = 5\n",
    "special_tokens = [\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"]\n",
    "word_to_index = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "index_to_word = {idx: token for idx, token in enumerate(special_tokens)}\n",
    "idx = 4\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= min_freq and word not in special_tokens:\n",
    "        word_to_index[word] = idx\n",
    "        index_to_word[idx] = word\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence(items):\n",
    "    sequences = {}\n",
    "    for image, captions in items.items():\n",
    "        sequences[image] = []\n",
    "        for caption in captions:\n",
    "            sequence = [1]\n",
    "            for word in caption.split(' '):\n",
    "                if word not in word_to_index:\n",
    "                    word = \"<UNK>\"\n",
    "                sequence.append(word_to_index[word])\n",
    "            if len(sequence) >= max_len:\n",
    "                sequence = sequence[:max_len - 1]\n",
    "                sequence.append(2)\n",
    "            elif len(sequence) < max_len:\n",
    "                sequence.append(2)\n",
    "                while(len(sequence) < max_len):\n",
    "                    sequence.append(0)\n",
    "            sequences[image].append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3c42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences = make_sequence(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08720d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GloVe embeddings already exist!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def download_glove_embeddings():\n",
    "    \"\"\"Download GloVe embeddings automatically\"\"\"\n",
    "    \n",
    "    embeddings_dir = 'embeddings'\n",
    "    if not os.path.exists(embeddings_dir):\n",
    "        os.makedirs(embeddings_dir)\n",
    "    \n",
    "    glove_file = os.path.join(embeddings_dir, 'glove.6B.300d.txt')\n",
    "    \n",
    "    if os.path.exists(glove_file):\n",
    "        print(\"âœ… GloVe embeddings already exist!\")\n",
    "        return glove_file\n",
    "    \n",
    "    print(\"ðŸ“¥ Downloading GloVe embeddings...\")\n",
    "    \n",
    "    # Download zip file\n",
    "    zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    zip_path = \"glove.6B.zip\"\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(zip_url, zip_path)\n",
    "        print(\"âœ… Download completed!\")\n",
    "        \n",
    "        # Extract specific file\n",
    "        print(\"ðŸ“‚ Extracting embeddings...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extract('glove.6B.300d.txt', embeddings_dir)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(zip_path)\n",
    "        print(f\"ðŸŽ‰ GloVe embeddings ready: {glove_file}\")\n",
    "        \n",
    "        return glove_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Download failed: {e}\")\n",
    "        print(\"ðŸ”— Manual download: http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "glove_path = download_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1446fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "file = open(glove_path, encoding=\"utf-8\")\n",
    "\n",
    "for line in file:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "file.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89aa33b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: (3040, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(word_to_index), embedding_dim))\n",
    "\n",
    "for word, idx in word_to_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "print(\"Embeddings matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f2cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pickle import dump\n",
    "\n",
    "# Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i\n",
    "os.makedirs(\"Processed Data\", exist_ok=True)\n",
    "\n",
    "# LÆ°u embedding matrix\n",
    "with open(\"Processed Data/embedding_matrix.pkl\", \"wb\") as f:\n",
    "    dump(embedding_matrix, f)\n",
    "\n",
    "# LÆ°u word mappings\n",
    "with open(\"Processed Data/word_to_index.pkl\", \"wb\") as f:\n",
    "    dump(word_to_index, f)\n",
    "\n",
    "with open(\"Processed Data/index_to_word.pkl\", \"wb\") as f:\n",
    "    dump(index_to_word, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5aef881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8091, 8091)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "images = {}\n",
    "captions = {}\n",
    "\n",
    "for image_path in all_sequences:\n",
    "    images[image_path] = Image.open(os.path.join(\"Flickr8k/Images\", image_path))\n",
    "    try:\n",
    "        captions[image_path].append(all_sequences[image_path])\n",
    "    except:\n",
    "        captions[image_path] = all_sequences[image_path]\n",
    "\n",
    "len(images), len(captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05038f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed Data/encoded_captions.pkl\", \"wb\") as f:\n",
    "    dump(captions, f)\n",
    "with open(\"Processed Data/images.pkl\", \"wb\") as f:\n",
    "    dump(images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load ResNet50, bá» fully connected cuá»‘i\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]  # Bá» fc\n",
    "resnet = torch.nn.Sequential(*modules)\n",
    "resnet.eval()\n",
    "\n",
    "# Transform cho áº£nh\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load images.pkl\n",
    "with open('Processed Data/images.pkl', 'rb') as f:\n",
    "    images = pickle.load(f)\n",
    "\n",
    "features = {}\n",
    "for img_name, img in images.items():\n",
    "    img = transform(img).unsqueeze(0)  # (1, 3, 224, 224)\n",
    "    with torch.no_grad():\n",
    "        feat = resnet(img).squeeze().numpy()  # (2048,)\n",
    "    features[img_name] = feat\n",
    "\n",
    "# LÆ°u láº¡i\n",
    "with open('Processed Data/image_features_resnet50.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
